source("/home/aivo/kool/R/mydistfun.R")
require(graphics)
# this is autogenerated test data
load(file="/home/aivo/kool/R/kNN_data1.RData")

my_kmedoids <- function(x, k, threshold=0.01) {
  calc.new.means <- function(x, clusters, k) {
    means <- c()
    for(c in 1:k)
    {
      # select the points with the matching cluster index
      cluster <- which(clusters == c)
      
      mt1 <- mean(x[cluster,1])
      mt2 <- mean(x[cluster,2])
      this_mean <- c(mt1, mt2)
      means <- c(means, this_mean)
    }
    means <- matrix(means, ncol=2, byrow=TRUE)
    return(means)
  }
  
  # helper fn to check how much was max centroid movement
  delta <- function(prev, new) {
    diff <- new - prev
    # transpose for the distance function
    max(euclidean.dist(t(diff)))
  }
  
  # Returns a vector with cluster numbers per each row in the input matrix
  calc.clusters <- function(x, means) {
    clusters = c()
    for(i in 1:nrow(x))
    {
      distances = c()
      # calculate closest cluster for each point
      for(j in 1:nrow(means))
      {
        # here is the primary distance function, uncomment to replace
        #dst <- mydistfun(x[i,], means[j,], "euclidean") 
        #dst <- mydistfun(x[i,], means[j,], "manhattan") 
        dst <- mydistfun(x[i,], means[j,], "Chebyshev") 
        #dst <- mydistfun(x[i,], means[j,], "Canberra")

        distances <- c(distances, dst)
      }
      min.distance <- min(distances)
      # find the index position in vector
      cluster <- match(min.distance, distances)
      clusters <- c(clusters, cluster)    
    }
    return (clusters)
  }
  # for reproducability
  set.seed(100)
  
  # Sample k random points from input matrix as initial centroids
  initial <- x[sample(nrow(x), k), ]
  
  means <- initial
  prev.means <- initial
  # Just to make sure that the while loop runs at least once
  difference <- threshold + 1 
  prev.means <- means
  iterations <- 0
  
  while(difference > threshold) {
    clusters = calc.clusters(x, means)
    prev.means <- means
    means <- calc.new.means(x, clusters, k)
    difference <- delta(prev.means, means)
    iterations <- iterations + 1
  }
  return(list(iters=iterations, cluster=clusters, centers=means))
}

k <- 3
threshold <- 0.01

# ensure that the input matrix only has 2 columns
results <- my_kmedoids(x[,1:2], k, threshold)

# create a results matrix with cluster info for filtering convinience
res_m <- cbind(x[,1:2], results$cluster) 

with_outliers <- c()

for (i in 1:k) {
  res_cluster <- res_m[res_m[,3] == i,]
  # calculate covariance matrix
  cov_matrix <- cov(res_cluster[,1:2])
  m_dist <- c()
  for (j in 1:nrow(res_cluster)) {
    m_dist <- rbind(m_dist, mahalanobis.dist(res_cluster[j, 1:2], results$centers[i,], cov_matrix))

  }
  # add a 4th column with a number representing number od standard deviations from centroid
  res_cluster <- cbind(res_cluster, m_dist/sd(m_dist))
  with_outliers <- rbind(with_outliers, res_cluster)
}

outlier_in_sd <- 5

no_outliers <- with_outliers[with_outliers[,4] < outlier_in_sd,]
outliers <- with_outliers[with_outliers[,4] >= outlier_in_sd,]


plot(no_outliers[,1:2], col = no_outliers[, 3])
points(results$centers, col = 1:k, pch = 8)
par(new=TRUE)
plot(outliers[,1:2], col="grey")



# Evaluate the algoritm

# initialize a vector of k distances to hold max distance between any points in cluster
cluster_max_dist <- rep(0, k)
# initialize a vector to hold average distances to all points in the same cluster
inner_cluster_avg <- c()
# initialize a vector to hold average distances to points in all other clusters
intra_cluster_avg <- c()


for (kk in 1:k) { #each cluster
  current_cluster <- no_outliers[no_outliers[,3] == kk,]
  other_clusters <- no_outliers[no_outliers[,3] != kk,]
  total_intra_cluster <- 0
  total_inner_cluster <- 0
  for (i in 1:nrow(current_cluster)) { #each row in this cluster
    for (j in 1:nrow(current_cluster)) { #each row again in this cluster
      if (i == j) {next}
      d <- mydistfun(current_cluster[i,], current_cluster[j,], "euclidean") 
      if (d > cluster_max_dist[kk]) {
        cluster_max_dist[kk] <- d
      }
      total_inner_cluster <- total_inner_cluster + d
    }
    for (jj in 1:nrow(other_clusters)) { #each row in other clusters
      d <- mydistfun(current_cluster[i,], other_clusters[jj,], "euclidean") 
      total_intra_cluster <- total_intra_cluster + d
    }
  }
  inner_cluster_avg <- cbind(inner_cluster_avg, total_inner_cluster/(nrow(current_cluster)-1))
  intra_cluster_avg <- cbind(intra_cluster_avg, total_intra_cluster/nrow(other_clusters))
}
# calculate averages across all clusters
inner_cluster_avg <- sum(inner_cluster_avg) / length(inner_cluster_avg)
intra_cluster_avg <- sum(intra_cluster_avg) / length(intra_cluster_avg)



min_dist <- function(centroids) {
  min <- Inf
  for (i in 1:nrow(centroids)) {
    for (j in 1:nrow(centroids)) {
      if (i == j) {next}
      d <- mydistfun(centroids[i,], centroids[j,], "euclidean") 
      if (d < min) {
        min <- d
      }
    }
  }
  return(min)
}

# https://en.wikipedia.org/wiki/Cluster_analysis
max_inter_cluster_distance <- max(cluster_max_dist)
min_inter_cluster_distance <- min_dist(results$centers)
dunn_index <- min_inter_cluster_distance / max_inter_cluster_distance
# https://en.wikipedia.org/wiki/Silhouette_(clustering)
silhouette_coefficient <- (intra_cluster_avg - inner_cluster_avg) / max(c(intra_cluster_avg, inner_cluster_avg))

# Results with different distance functions

# euclidean distance
# silhouette_coefficient: 0.668
# Dunn index: 0.582

# manhattan distance
# silhouette_coefficient: 0.661
# Dunn index: 0.631
# clustering is visibly worse than with euclidean

# Chebyshev distance
# silhouette_coefficient: 0.667
# Dunn index: 0.580
# clustering is visibly worse than with euclidean

# Canberra distance
# silhouette_coefficient: 0.504
# Dunn index: 0.155
# clustering is visibly worse than with euclidean
